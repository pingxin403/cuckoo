# Prometheus Alerting Rules for IM Gateway Service
# These rules define alerts for critical system conditions

groups:
  - name: im_gateway_alerts
    interval: 30s
    rules:
      # P1 Alert: High P99 Latency
      - alert: HighMessageDeliveryLatency
        expr: histogram_quantile(0.99, rate(im_gateway_message_delivery_latency_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: P1
          service: im-gateway-service
          component: message-delivery
        annotations:
          summary: "High P99 message delivery latency detected"
          description: "P99 message delivery latency is {{ $value | humanizeDuration }} (threshold: 500ms) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/high-latency"
          dashboard_url: "http://grafana:3000/d/im-gateway-messages"

      # Circuit Breaker: High Message Loss Rate
      - alert: HighMessageLossRate
        expr: |
          (
            rate(im_gateway_messages_failed_total[5m]) 
            / 
            (rate(im_gateway_messages_delivered_total[5m]) + rate(im_gateway_messages_failed_total[5m]))
          ) > 0.0001
        for: 2m
        labels:
          severity: critical
          service: im-gateway-service
          component: message-delivery
          circuit_breaker: "true"
        annotations:
          summary: "High message loss rate detected - Circuit breaker triggered"
          description: "Message loss rate is {{ $value | humanizePercentage }} (threshold: 0.01%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/message-loss"
          dashboard_url: "http://grafana:3000/d/im-gateway-health"
          action: "Circuit breaker should be triggered to prevent further message loss"

      # P2 Alert: High ACK Timeout Rate
      - alert: HighAckTimeoutRate
        expr: |
          (
            rate(im_gateway_ack_timeouts_total[5m]) 
            / 
            rate(im_gateway_messages_delivered_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: P2
          service: im-gateway-service
          component: message-delivery
        annotations:
          summary: "High ACK timeout rate detected"
          description: "ACK timeout rate is {{ $value | humanizePercentage }} (threshold: 5%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/ack-timeout"
          dashboard_url: "http://grafana:3000/d/im-gateway-messages"

      # Connection Health Alerts
      - alert: HighConnectionErrorRate
        expr: |
          (
            rate(im_gateway_connection_errors_total[5m]) 
            / 
            rate(im_gateway_total_connections_total[5m])
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          service: im-gateway-service
          component: connections
        annotations:
          summary: "High connection error rate detected"
          description: "Connection error rate is {{ $value | humanizePercentage }} (threshold: 10%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/connection-errors"
          dashboard_url: "http://grafana:3000/d/im-gateway-connections"

      - alert: TooManyActiveConnections
        expr: im_gateway_active_connections > 100000
        for: 5m
        labels:
          severity: warning
          service: im-gateway-service
          component: connections
        annotations:
          summary: "Gateway node approaching connection limit"
          description: "Active connections ({{ $value }}) approaching limit (100K) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/scale-out"
          dashboard_url: "http://grafana:3000/d/im-gateway-connections"
          action: "Consider scaling out gateway nodes"

      # Offline Queue Alerts
      - alert: HighOfflineQueueBacklog
        expr: im_gateway_offline_queue_size > 10000
        for: 10m
        labels:
          severity: warning
          service: im-gateway-service
          component: offline-queue
        annotations:
          summary: "High offline message queue backlog"
          description: "Offline queue size is {{ $value }} messages (threshold: 10K) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/offline-backlog"
          dashboard_url: "http://grafana:3000/d/im-gateway-messages"
          action: "Check offline worker processing rate"

      # Cache Performance Alerts
      - alert: LowCacheHitRate
        expr: |
          (
            rate(im_gateway_cache_hits_total[5m]) 
            / 
            (rate(im_gateway_cache_hits_total[5m]) + rate(im_gateway_cache_misses_total[5m]))
          ) < 0.7
        for: 10m
        labels:
          severity: warning
          service: im-gateway-service
          component: cache
        annotations:
          summary: "Low cache hit rate detected"
          description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 70%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/cache-performance"
          dashboard_url: "http://grafana:3000/d/im-gateway-health"
          action: "Review cache configuration and TTL settings"

      # Duplication Rate Alert
      - alert: HighMessageDuplicationRate
        expr: |
          (
            rate(im_gateway_duplicate_messages_total[5m]) 
            / 
            rate(im_gateway_messages_delivered_total[5m])
          ) > 0.01
        for: 10m
        labels:
          severity: warning
          service: im-gateway-service
          component: deduplication
        annotations:
          summary: "High message duplication rate detected"
          description: "Message duplication rate is {{ $value | humanizePercentage }} (threshold: 1%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/duplication"
          dashboard_url: "http://grafana:3000/d/im-gateway-health"
          action: "Check Redis deduplication service health"

      # Service Availability Alert
      - alert: IMGatewayServiceDown
        expr: up{job="im-gateway-service"} == 0
        for: 1m
        labels:
          severity: critical
          service: im-gateway-service
          component: availability
        annotations:
          summary: "IM Gateway Service is down"
          description: "IM Gateway Service {{ $labels.instance }} is not responding to health checks"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/service-down"
          dashboard_url: "http://grafana:3000/d/im-gateway-health"
          action: "Immediate investigation required"

      # Group Message Performance
      - alert: HighGroupMessageFanoutLatency
        expr: |
          rate(im_gateway_group_members_fanout_total[5m]) 
          / 
          rate(im_gateway_group_messages_delivered_total[5m]) > 1000
        for: 10m
        labels:
          severity: warning
          service: im-gateway-service
          component: group-messages
        annotations:
          summary: "High average group size detected"
          description: "Average group fanout is {{ $value }} members (threshold: 1000) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/large-groups"
          dashboard_url: "http://grafana:3000/d/im-gateway-messages"
          action: "Review large group optimization settings"

  # OpenTelemetry Instrumentation Health
  - name: otel_instrumentation_alerts
    interval: 30s
    rules:
      - alert: HighOTelMetricsExportFailures
        expr: rate(otelcol_exporter_send_failed_metric_points[5m]) > 10
        for: 5m
        labels:
          severity: warning
          service: otel-collector
          component: metrics-export
        annotations:
          summary: "High OpenTelemetry metrics export failure rate"
          description: "OTel Collector is failing to export metrics at {{ $value }} failures/sec"
          runbook_url: "https://wiki.example.com/runbooks/otel/export-failures"
          action: "Check OTel Collector logs and backend connectivity"

      - alert: OTelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 2m
        labels:
          severity: critical
          service: otel-collector
          component: availability
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OTel Collector is not responding to health checks"
          runbook_url: "https://wiki.example.com/runbooks/otel/collector-down"
          action: "Restart OTel Collector immediately"


  # SLO and Error Budget Alerts
  - name: slo_alerts
    interval: 30s
    rules:
      # Error Budget 50% Consumed
      - alert: ErrorBudget50PercentConsumed
        expr: |
          (1 - avg_over_time(up{job="im-gateway-service"}[30d])) / 0.0005 > 0.5
        for: 5m
        labels:
          severity: warning
          service: im-gateway-service
          component: slo
        annotations:
          summary: "50% of monthly error budget consumed"
          description: "Error budget consumption: {{ $value | humanizePercentage }} (threshold: 50%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/error-budget"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Review recent incidents and plan improvements"

      # Error Budget 80% Consumed (Critical)
      - alert: ErrorBudget80PercentConsumed
        expr: |
          (1 - avg_over_time(up{job="im-gateway-service"}[30d])) / 0.0005 > 0.8
        for: 5m
        labels:
          severity: critical
          service: im-gateway-service
          component: slo
        annotations:
          summary: "80% of monthly error budget consumed - Circuit breaker recommended"
          description: "Error budget consumption: {{ $value | humanizePercentage }} (threshold: 80%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/error-budget"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Consider enabling circuit breaker to preserve remaining budget"

      # Fast SLO Burn (1 hour window)
      - alert: SLOFastBurn
        expr: |
          (
            1 - (
              sum(rate(im_gateway_messages_delivered_total[1h]))
              /
              (sum(rate(im_gateway_messages_delivered_total[1h])) + 
               sum(rate(im_gateway_messages_failed_total[1h])))
            )
          ) > (0.0001 * 14.4)
        for: 2m
        labels:
          severity: critical
          service: im-gateway-service
          component: slo
          burn_rate: fast
        annotations:
          summary: "Fast SLO burn detected - 5% of monthly budget in 1 hour"
          description: "Current error rate: {{ $value | humanizePercentage }} (threshold: 0.144%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/slo-burn"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Immediate investigation required - burning budget 14.4x faster than allowed"

      # Slow SLO Burn (6 hour window)
      - alert: SLOSlowBurn
        expr: |
          (
            1 - (
              sum(rate(im_gateway_messages_delivered_total[6h]))
              /
              (sum(rate(im_gateway_messages_delivered_total[6h])) + 
               sum(rate(im_gateway_messages_failed_total[6h])))
            )
          ) > (0.0001 * 6)
        for: 15m
        labels:
          severity: warning
          service: im-gateway-service
          component: slo
          burn_rate: slow
        annotations:
          summary: "Slow SLO burn detected - 2.5% of monthly budget in 6 hours"
          description: "Current error rate: {{ $value | humanizePercentage }} (threshold: 0.06%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/slo-burn"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Investigation required within 1 hour - burning budget 6x faster than allowed"

      # Availability SLO Violation
      - alert: AvailabilitySLOViolation
        expr: avg_over_time(up{job="im-gateway-service"}[30d]) < 0.9995
        for: 5m
        labels:
          severity: critical
          service: im-gateway-service
          component: slo
        annotations:
          summary: "Availability SLO violated"
          description: "30-day availability: {{ $value | humanizePercentage }} (target: 99.95%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/slo-violation"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Review incidents and implement improvements to restore SLO compliance"

      # Latency SLO Violation
      - alert: LatencySLOViolation
        expr: |
          histogram_quantile(0.99, 
            rate(im_gateway_message_delivery_latency_seconds_bucket[30d])
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          service: im-gateway-service
          component: slo
        annotations:
          summary: "Latency SLO violated"
          description: "30-day P99 latency: {{ $value | humanizeDuration }} (target: < 200ms)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/slo-violation"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Investigate latency issues and optimize performance"

      # Success Rate SLO Violation
      - alert: SuccessRateSLOViolation
        expr: |
          (
            sum(rate(im_gateway_messages_delivered_total[30d]))
            /
            (sum(rate(im_gateway_messages_delivered_total[30d])) + 
             sum(rate(im_gateway_messages_failed_total[30d])))
          ) < 0.9999
        for: 5m
        labels:
          severity: critical
          service: im-gateway-service
          component: slo
        annotations:
          summary: "Success rate SLO violated"
          description: "30-day success rate: {{ $value | humanizePercentage }} (target: 99.99%)"
          runbook_url: "https://wiki.example.com/runbooks/im-gateway/slo-violation"
          dashboard_url: "http://grafana:3000/d/im-gateway-slo"
          action: "Investigate message delivery failures and implement fixes"

  # Flash Sale System Alerts
  - name: flash_sale_alerts
    interval: 30s
    rules:
      # Threshold Alert: High Failure Rate
      - alert: FlashSaleHighFailureRate
        expr: |
          (
            rate(flash_sale_requests_failure[5m])
            /
            rate(flash_sale_requests_total[5m])
          ) > 0.05
        for: 3m
        labels:
          severity: critical
          service: flash-sale-service
          component: seckill
        annotations:
          summary: "High flash sale failure rate detected"
          description: "Flash sale failure rate is {{ $value | humanizePercentage }} (threshold: 5%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-failure-rate"
          dashboard_url: "http://grafana:3000/d/flash-sale-overview"
          action: "Check Redis connectivity, inventory service health, and system logs"

      # Threshold Alert: High Response Time (P99)
      - alert: FlashSaleHighResponseTime
        expr: histogram_quantile(0.99, rate(flash_sale_request_duration_bucket[5m])) > 0.2
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: performance
        annotations:
          summary: "High P99 response time detected"
          description: "P99 response time is {{ $value | humanizeDuration }} (threshold: 200ms) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-latency"
          dashboard_url: "http://grafana:3000/d/flash-sale-performance"
          action: "Check Redis latency, Kafka producer lag, and system resource utilization"

      # Threshold Alert: Critical Response Time (P99)
      - alert: FlashSaleCriticalResponseTime
        expr: histogram_quantile(0.99, rate(flash_sale_request_duration_bucket[5m])) > 0.5
        for: 2m
        labels:
          severity: critical
          service: flash-sale-service
          component: performance
        annotations:
          summary: "Critical P99 response time detected"
          description: "P99 response time is {{ $value | humanizeDuration }} (threshold: 500ms) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/critical-latency"
          dashboard_url: "http://grafana:3000/d/flash-sale-performance"
          action: "Immediate investigation required - consider enabling circuit breaker"

      # Threshold Alert: Low Inventory Warning
      - alert: FlashSaleLowInventory
        expr: flash_sale_inventory_remaining < 100
        for: 1m
        labels:
          severity: warning
          service: flash-sale-service
          component: inventory
        annotations:
          summary: "Low inventory detected for SKU"
          description: "Remaining inventory for SKU {{ $labels.sku_id }} is {{ $value }} units (threshold: 100)"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/low-inventory"
          dashboard_url: "http://grafana:3000/d/flash-sale-inventory"
          action: "Prepare for sold-out notification and queue closure"

      # Threshold Alert: Inventory Depleted
      - alert: FlashSaleInventoryDepleted
        expr: flash_sale_inventory_remaining == 0
        for: 30s
        labels:
          severity: info
          service: flash-sale-service
          component: inventory
        annotations:
          summary: "Inventory depleted for SKU"
          description: "SKU {{ $labels.sku_id }} is now sold out"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/sold-out"
          dashboard_url: "http://grafana:3000/d/flash-sale-inventory"
          action: "Verify sold-out notification sent to queued users"

      # Threshold Alert: High Queue Length
      - alert: FlashSaleHighQueueLength
        expr: flash_sale_queue_length > 10000
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: queue
        annotations:
          summary: "High queue length detected"
          description: "Queue length is {{ $value }} (threshold: 10,000) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-queue"
          dashboard_url: "http://grafana:3000/d/flash-sale-queue"
          action: "Monitor token bucket rate and consider increasing throughput"

      # Threshold Alert: Critical Queue Length
      - alert: FlashSaleCriticalQueueLength
        expr: flash_sale_queue_length > 50000
        for: 2m
        labels:
          severity: critical
          service: flash-sale-service
          component: queue
        annotations:
          summary: "Critical queue length detected"
          description: "Queue length is {{ $value }} (threshold: 50,000) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/critical-queue"
          dashboard_url: "http://grafana:3000/d/flash-sale-queue"
          action: "Consider rejecting new requests or scaling up service instances"

      # Threshold Alert: High Token Rejection Rate
      - alert: FlashSaleHighTokenRejectionRate
        expr: |
          (
            rate(flash_sale_queue_tokens_rejected[5m])
            /
            (rate(flash_sale_queue_tokens_acquired[5m]) + rate(flash_sale_queue_tokens_rejected[5m]))
          ) > 0.8
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: queue
        annotations:
          summary: "High token rejection rate detected"
          description: "Token rejection rate is {{ $value | humanizePercentage }} (threshold: 80%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-rejection"
          dashboard_url: "http://grafana:3000/d/flash-sale-queue"
          action: "Review token bucket configuration and rate limits"

      # Threshold Alert: High Inventory Deduction Latency
      - alert: FlashSaleHighInventoryDeductionLatency
        expr: histogram_quantile(0.99, rate(flash_sale_inventory_deduction_duration_bucket[5m])) > 0.05
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: inventory
        annotations:
          summary: "High inventory deduction latency detected"
          description: "P99 inventory deduction latency is {{ $value | humanizeDuration }} (threshold: 50ms) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/inventory-latency"
          dashboard_url: "http://grafana:3000/d/flash-sale-inventory"
          action: "Check Redis performance and network latency"

      # Threshold Alert: High Inventory Rollback Rate
      - alert: FlashSaleHighRollbackRate
        expr: |
          (
            rate(flash_sale_inventory_rollbacks_total[10m])
            /
            rate(flash_sale_inventory_deductions_total[10m])
          ) > 0.3
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: inventory
        annotations:
          summary: "High inventory rollback rate detected"
          description: "Inventory rollback rate is {{ $value | humanizePercentage }} (threshold: 30%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-rollback"
          dashboard_url: "http://grafana:3000/d/flash-sale-inventory"
          action: "Investigate order timeout issues and payment processing delays"

      # Fault Alert: Flash Sale Service Down
      - alert: FlashSaleServiceDown
        expr: up{job="flash-sale-service"} == 0
        for: 1m
        labels:
          severity: critical
          service: flash-sale-service
          component: availability
        annotations:
          summary: "Flash Sale Service is down"
          description: "Flash Sale Service {{ $labels.instance }} is not responding to health checks"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/service-down"
          dashboard_url: "http://grafana:3000/d/flash-sale-health"
          action: "Immediate investigation required - check service logs and restart if necessary"

      # Fault Alert: Redis Connection Failure
      - alert: FlashSaleRedisConnectionFailure
        expr: |
          rate(flash_sale_requests_failure{error_type="redis_error"}[5m]) > 10
        for: 2m
        labels:
          severity: critical
          service: flash-sale-service
          component: redis
        annotations:
          summary: "Redis connection failures detected"
          description: "Redis connection failure rate is {{ $value }} errors/sec for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/redis-failure"
          dashboard_url: "http://grafana:3000/d/flash-sale-health"
          action: "Check Redis service health, network connectivity, and connection pool configuration"

      # Fault Alert: Kafka Producer Failure
      - alert: FlashSaleKafkaProducerFailure
        expr: |
          rate(flash_sale_requests_failure{error_type="kafka_error"}[5m]) > 5
        for: 2m
        labels:
          severity: critical
          service: flash-sale-service
          component: kafka
        annotations:
          summary: "Kafka producer failures detected"
          description: "Kafka producer failure rate is {{ $value }} errors/sec for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/kafka-failure"
          dashboard_url: "http://grafana:3000/d/flash-sale-health"
          action: "Check Kafka broker health, network connectivity, and producer configuration"

      # Performance Alert: Low Success Rate
      - alert: FlashSaleLowSuccessRate
        expr: |
          (
            rate(flash_sale_requests_success[5m])
            /
            rate(flash_sale_requests_total[5m])
          ) < 0.9
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: seckill
        annotations:
          summary: "Low flash sale success rate detected"
          description: "Flash sale success rate is {{ $value | humanizePercentage }} (threshold: 90%) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/low-success-rate"
          dashboard_url: "http://grafana:3000/d/flash-sale-overview"
          action: "Review error logs and check system dependencies (Redis, Kafka, MySQL)"

      # Capacity Alert: High Request Rate
      - alert: FlashSaleHighRequestRate
        expr: rate(flash_sale_requests_total[1m]) > 100000
        for: 2m
        labels:
          severity: warning
          service: flash-sale-service
          component: capacity
        annotations:
          summary: "High request rate detected"
          description: "Request rate is {{ $value }} req/sec (threshold: 100K QPS) for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/high-qps"
          dashboard_url: "http://grafana:3000/d/flash-sale-overview"
          action: "Monitor system resources and consider scaling horizontally if sustained"

      # Data Consistency Alert: Inventory Deduction Anomaly
      - alert: FlashSaleInventoryDeductionAnomaly
        expr: |
          rate(flash_sale_inventory_deductions_total[5m]) > 
          rate(flash_sale_requests_success[5m]) * 1.1
        for: 5m
        labels:
          severity: warning
          service: flash-sale-service
          component: consistency
        annotations:
          summary: "Inventory deduction anomaly detected"
          description: "Inventory deductions ({{ $value }}) exceed successful requests by >10% for service {{ $labels.instance }}"
          runbook_url: "https://wiki.example.com/runbooks/flash-sale/deduction-anomaly"
          dashboard_url: "http://grafana:3000/d/flash-sale-inventory"
          action: "Investigate potential double-deduction or reconciliation issues"
